{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Réseau de neurones multicouches </font> à Feedforward\n",
    "---\n",
    "Les réseaux neuronaux sont nés du fait que tout ne peut pas être approximé par une régression linéaire. Il peut y avoir des formes potentiellement complexes dans les données qui ne peuvent être approximées que par des fonctions complexes, comme c'est le cas pour notre ensemble de données du code ASCII. Plus la fonction est complexe, meilleure est la précision des prédictions. \n",
    "\n",
    "Un réseau multicouches à feedforward consiste en un ensemble de neurones qui sont logiquement disposés en deux ou plusieurs couches. Il existe une couche d'entrée et une couche de sortie, chacune contenant au moins un neurone. Les neurones de la couche d'entrée sont hypothétiques dans la mesure où ils n'ont pas eux-mêmes d'entrées et où ils ne traitent rien, leur activation (sortie) est définie par \n",
    "l'entrée du réseau. Il y a généralement une ou plusieurs couches \"cachées\" entre les couches d'entrée et de sortie.\n",
    "\n",
    "Le terme \"feedforward\" signifie que l'information circule dans un seul sens.  L'entrée dans les neurones de chaque couche provient exclusivement des sorties des neurones des couches précédentes, et les sorties de ces neurones passent exclusivement aux neurones des couches suivantes, la sortie de chaque neurone du réseau est fonction de l'entrée de ce neurone.\n",
    "\n",
    "![1](images/neuron.png)\n",
    "\n",
    "Les réseaux neuronaux sont composés de simples blocs de construction appelés neurones. Un neurone est une fonction mathématique qui prend des données comme entrée, effectue une transformation sur celles-ci et produit une sortie. Cela signifie que les neurones peuvent représenter n'importe quelle fonction mathématique ; cependant, dans les réseaux neuronaux, nous utilisons généralement des fonctions non linéaires.\n",
    "\n",
    "En regardant le neurone ci-dessus, vous pouvez voir qu'il est composé de deux parties principales : la sommation et la fonction d'activation. Un neurone prend des données (x₁, x₂, x₃) comme entrée, multiplie chacune d'entre elles par un poids spécifique (w₁, w₂, w₃), puis transmet le résultat à une fonction non linéaire appelée fonction d'activation pour produire une sortie (y).\n",
    "\n",
    "Dans ce tutoriel, la tâche consiste à créer un réseau de neurones pour classer les caractères du code ASCII à partir de ses bits. Nous avons un ensemble de entraînement, pour entraîner le classificateur et un ensemble de test, pour tester le résultat de l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=\"red\">1 | LES </font>DONNÉES\n",
    "---\n",
    "La première chose que nous allons faire est d'importer les bibliothèques nécessaires et de charger notre ensemble de données. Les librairies que nous utiliserons sont :\n",
    "* [numpy](https://numpy.org/) : Extension destinée à manipuler des matrices ou tableaux multidimensionnels ainsi que des fonctions mathématiques opérant sur ces tableaux.\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html) : Ce module permet de sauvegarder dans un fichier, au format binaire,  n'importe quel objet Python.\n",
    "* [tabulate](https://pypi.org/project/tabulate/) : Pour afficher des tableaux d'une manière facile à lire. \n",
    "* [mathplotlib](https://matplotlib.org/) : C'est une bibliothèque pour tracer des graphiques en Python.\n",
    "\n",
    "Pour une explication plus détaillée de l'ensemble des données, veuillez vous référer au notebook correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ascii_features.pickle\", \"rb\") as db:\n",
    "    features = pickle.load(db)\n",
    "    \n",
    "with open(\"ascii_outputs.pickle\", \"rb\") as db:\n",
    "    outputs = pickle.load(db)\n",
    "\n",
    "X_train, X_test = features['TRAIN'], features['TEST']\n",
    "outputs_train, outputs_test, classes = outputs['TRAIN'], outputs['TEST'], outputs['CLASSES']\n",
    "\n",
    "labels_train = [string_label for label in outputs_train for string_label in label.keys()]\n",
    "y_train = np.array([binary_label for label in outputs_train for binary_label in label.values()])\n",
    "\n",
    "labels_test = [string_label for label in outputs_test for string_label in label.keys()]\n",
    "y_test = np.array([binary_label for label in outputs_test for binary_label in label.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=\"red\">2 | LE </font>MODÈLE\n",
    "---\n",
    "Maintenant que vous avez chargé et préparé l'ensemble des données, commençons à construire le réseau de neurones pour faire des prédictions. Pour que les choses restent relativement simples, vous allez concevoir et coder un réseau de neurones à une couche cachée. Vous trouverez ci-dessous un aperçu de l'architecture :\n",
    "\n",
    "![2](images/nn_arch.png)\n",
    "\n",
    "La première couche est appelée couche d'entrée, et le nombre de nœuds dépendra du nombre de caractéristiques présentes dans votre ensemble de données. Dans notre cas, il y aura 8 nœuds car nous avons 8 primitives (bits).\n",
    "\n",
    "La dernière couche du réseau de neurones est appelée la couche de sortie, et son nombre dépend de ce que vous essayez de prédire. Pour les tâches de régression et de classification binaire, vous pouvez utiliser un seul nœud, tandis que pour les problèmes multi-classes (comme le notre), vous utiliserez plusieurs nœuds, en fonction du nombre de classes.\n",
    "\n",
    "Les couches situées entre la couche d'entrée et la couche de sortie sont celles où la magie opère, on les appelle les couches cachées. Les couches cachées peuvent être aussi profondes ou larges que vous le souhaitez, et si un réseau plus profond est préférable, le temps de calcul augmente également à mesure que vous vous enfoncez.\n",
    "\n",
    "Le réseau neuronal ci-dessus aura une couche cachée et une couche de sortie. La couche d'entrée aura 8 nœuds car nous avons 8 primitives. La couche cachée peut accepter un nombre quelconque de nœuds, mais vous commencerez avec 6, et la couche de sortie, qui fait les prédictions, aura 5 nœuds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">2.1 - Implémentation</font>\n",
    "---\n",
    "Le réseau neuronal artificiel est un algorithme d'apprentissage supervisé qui exploite un mélange de multiples hyperparamètres qui permettent d'approcher une relation complexe entre l'entrée et la sortie. \n",
    "\n",
    "Voici quelques-uns des hyperparamètres du réseau neuronal artificiel :\n",
    "- Nombre de couches cachées\n",
    "- Nombre d'unités cachées\n",
    "- Fonction d'activation\n",
    "- Taux d'apprentissage\n",
    "\n",
    "### **Poids et biais**\n",
    "\n",
    "Nous allons construire notre modèle à l'intérieur d'une classe appelée `NeuralNetwork`. Le constructeur prend des paramètres qui seront utilisés pour initialiser les poids et le biais, tels que le taux d'apprentissage, le nombre de neurones dans la couche d'entrée, le nombre de neurones dans la couche cachée et le nombre de neurones dans la couche de sortie.\n",
    "\n",
    "Les poids sont des valeurs qui contrôlent la force de la connexion entre deux neurones. En d'autres termes, les entrées sont généralement multipliées par des poids, et cela définit l'influence de l'entrée sur la sortie. Les termes de biais sont des constantes supplémentaires attachées aux neurones et ajoutées à l'entrée pondérée avant que la fonction d'activation ne soit appliquée. Les termes de biais aident les modèles à représenter des modèles qui ne passent pas nécessairement par l'origine. \n",
    "\n",
    "Dans le bloc de code ci-dessous, vous allez créer la classe `NeuralNetwork` et initialiser les poids et biais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, nb_input_nodes, nb_hidden_nodes, nb_output_nodes, learning_rate):\n",
    "        #  >>> HYPERPARAMETERS <<<\n",
    "        self.nb_input_nodes = nb_input_nodes\n",
    "        self.nb_output_nodes = nb_output_nodes\n",
    "        self.nb_hidden_nodes = nb_hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # >>> WEIGHTS + BIASES <<<\n",
    "        # Weight matrix from input to hidden layer\n",
    "        self.W1 = np.random.randn(self.nb_input_nodes, self.nb_hidden_nodes)\n",
    "        # Bias from input to hidden layer\n",
    "        self.b1 = np.ones((self.nb_hidden_nodes))\n",
    "\n",
    "        # Weight matrix from hidden to output layer\n",
    "        self.W2 = np.random.randn(self.nb_hidden_nodes, self.nb_output_nodes)\n",
    "        # Bias from hidden to output layer\n",
    "        self.b2 = np.ones((self.nb_output_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous remarquerez qu'il y a deux tableaux de poids et de biais. Le premier tableau de poids (W1) aura des dimensions de 8 par 6, parce que vous avez 8 primitives d'entrée et 6 nœuds cachés, tandis que le premier biais (b1) sera un vecteur de taille 6 parce que vous avez 6 nœuds cachés.\n",
    "\n",
    "Le deuxième tableau de pondération (W2) sera un tableau de 6 par 5 dimensions parce que vous avez 6 nœuds cachés et 5 nœuds de sortie, et enfin, le deuxième biais (b2) sera un vecteur de taille 5 parce que vous avez 5 sorties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function d'activation**\n",
    "\n",
    "Maintenant que vous avez initialisé les poids et les biais, définissons la fonction d'activation. Une fonction d'activation est ce qui rend un réseau de neurones capable d'apprendre des fonctions non linéaires complexes. Les fonctions non linéaires sont difficiles à apprendre pour les algorithmes d'apprentissage machine traditionnels comme la logistique et la régression linéaire. La fonction d'activation est ce qui rend un réseau de neurones capable de comprendre ces fonctions.\n",
    "\n",
    "La fonction d'activation est calculée par chaque nœud dans les couches cachées d'un réseau neuronal.Cela signifie que vous devrez faire passer les sommes pondérées par la fonction d'activation.\n",
    "\n",
    "Il existe de nombreux types de fonctions d'activation utilisées dans les réseaux neuronaux, la plus populaire étant le sigmoïde. L'une des raisons d'utiliser la fonction sigmoïde est due à ses propriétés mathématiques, dans notre cas, à ses dérivées. Lorsque le réseau de neurones fait le backpropagation pour apprendre et mettre à jour les poids, nous utiliserons sa dérivée. À continuation on implémente la fonction `sigmoid` et `derivative_sigmoid`\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma(x) = \\frac{1}{1+\\exp^{-x}} \\\\\n",
    "    \\sigma'(x) = \\sigma(x) \\cdot (1-\\sigma(x))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, nb_input_nodes, nb_hidden_nodes, nb_output_nodes, learning_rate):\n",
    "        #  >>> HYPERPARAMETERS <<<\n",
    "        self.nb_input_nodes = nb_input_nodes\n",
    "        self.nb_output_nodes = nb_output_nodes\n",
    "        self.nb_hidden_nodes = nb_hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # >>> WEIGHTS + BIASES <<<\n",
    "        # Weight matrix from input to hidden layer\n",
    "        self.W1 = np.random.randn(self.nb_input_nodes, self.nb_hidden_nodes)\n",
    "        # Bias from input to hidden layer\n",
    "        self.b1 = np.ones((self.nb_hidden_nodes))\n",
    "\n",
    "        # Weight matrix from hidden to output layer\n",
    "        self.W2 = np.random.randn(self.nb_hidden_nodes, self.nb_output_nodes)\n",
    "        # Bias from hidden to output layer\n",
    "        self.b2 = np.ones((self.nb_output_nodes))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    def derivative_sigmoid(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Propagation en avant**\n",
    "\n",
    "La propagation en avant est le nom donné à la série de calculs effectués par le réseau de neurones avant qu'une prédiction ne soit faite. Le processus de feed-forward est assez simple. La formule $z = (weights \\cdot input) + b$ calcule $z$ qui est passé dans les couches, qui contient des fonctions d'activation spécifiques. Ces fonctions d'activation produisent la sortie $f(z)$. La sortie de la couche actuel sera l'entrée de la couche suivant et ainsi de suite.\n",
    "\n",
    "Dans votre réseau à deux couches, vous effectuerez le calcul suivant pour la propagation directe :\n",
    "\n",
    "* Calculez la somme pondérée entre les poids de l'entrée et ceux de la première couche, puis ajoutez le biais : **Z2 = (W1 * X) + b**\n",
    "* Faites passer le résultat par la fonction d'activation sigmoïde : **A2 = sigmoïde(Z2)**\n",
    "* Calculez la somme pondérée entre la sortie (A2) de l'étape précédente et les poids de la deuxième couche - ajoutez également le biais : **Z3 = (W2 * A2) + b2**\n",
    "* Calculer la fonction de sortie en faisant passer le résultat par une fonction sigmoïde : **A3 = sigmoïde(Z3)**\n",
    "* Et enfin, calculer la perte entre le résultat prévu et les étiquettes réelles : **perte(A3, Y)**\n",
    "\n",
    "À continuation, l'implémentation de la fonction `forward` d'accord à la formule précédente : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, nb_input_nodes, nb_hidden_nodes, nb_output_nodes, learning_rate):\n",
    "        #  >>> HYPERPARAMETERS <<<\n",
    "        self.nb_input_nodes = nb_input_nodes\n",
    "        self.nb_output_nodes = nb_output_nodes\n",
    "        self.nb_hidden_nodes = nb_hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # >>> WEIGHTS + BIASES <<<\n",
    "        # Weight matrix from input to hidden layer\n",
    "        self.W1 = np.random.randn(self.nb_input_nodes, self.nb_hidden_nodes)\n",
    "        # Bias from input to hidden layer\n",
    "        self.b1 = np.ones((self.nb_hidden_nodes))\n",
    "\n",
    "        # Weight matrix from hidden to output layer\n",
    "        self.W2 = np.random.randn(self.nb_hidden_nodes, self.nb_output_nodes)\n",
    "        # Bias from hidden to output layer\n",
    "        self.b2 = np.ones((self.nb_output_nodes))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    def derivative_sigmoid(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through our network\n",
    "        self.A1 = X\n",
    "        self.Z2 = np.dot(self.A1, self.W1) + self.b1\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        self.Z3 = np.dot(self.A2, self.W2) + self.b2\n",
    "        self.A3 = self.sigmoid(self.Z3)\n",
    "        return self.A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mesurer l'erreur**\n",
    "\n",
    "La fonction de perte est un moyen de mesurer la performance de votre réseau par rapport aux valeurs réelles.\n",
    "\n",
    "La fonction de perte la plus couramment utilisée est l'erreur quadratique moyenne. Comme nous traitons d'un problème de classification multi-classes, le résultat sera une distribution de probabilité. nous devons la comparer à nos valeurs réelles, qui sont aussi une distribution de probabilité, et trouver l'erreur.\n",
    "\n",
    "Nous allons utiliser la fonction de Cross-Entropy pour évaluer l’erreur. Cette fonction mesure les performances d'un modèle de classification dont le résultat est une probabilité.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    Cross Entropy = -\\sum_{i=1}^{C} y_{i}\\log(\\hat{y}_i)\n",
    "\\end{equation}\n",
    "\n",
    "Ou :\n",
    "\n",
    "* $C$ : C'est le nombre de classes\n",
    "* $y$ : Étiquettes de classes pour chaque exemple\n",
    "* $\\hat{y}_i$ : C'est la probabilité prédite que l'entrée est de la classe $c$\n",
    "\n",
    "À conitnuation, on fait l'implémentation en python de la fonction `entropy_loss` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, nb_input_nodes, nb_hidden_nodes, nb_output_nodes, learning_rate):\n",
    "        #  >>> HYPERPARAMETERS <<<\n",
    "        self.nb_input_nodes = nb_input_nodes\n",
    "        self.nb_output_nodes = nb_output_nodes\n",
    "        self.nb_hidden_nodes = nb_hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # >>> WEIGHTS + BIASES <<<\n",
    "        # Weight matrix from input to hidden layer\n",
    "        self.W1 = np.random.randn(self.nb_input_nodes, self.nb_hidden_nodes)\n",
    "        # Bias from input to hidden layer\n",
    "        self.b1 = np.ones((self.nb_hidden_nodes))\n",
    "\n",
    "        # Weight matrix from hidden to output layer\n",
    "        self.W2 = np.random.randn(self.nb_hidden_nodes, self.nb_output_nodes)\n",
    "        # Bias from hidden to output layer\n",
    "        self.b2 = np.ones((self.nb_output_nodes))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    def derivative_sigmoid(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def entropy_loss(self, y, y_pred):\n",
    "        # Prevent taking the log of 0\n",
    "        eps = np.finfo(float).eps\n",
    "        return -np.sum(y * np.log(y_pred + eps))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through our network\n",
    "        self.A1 = X\n",
    "        self.Z2 = np.dot(self.A1, self.W1) + self.b1\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        self.Z3 = np.dot(self.A2, self.W2) + self.b2\n",
    "        self.A3 = self.sigmoid(self.Z3)\n",
    "        return self.A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Propagation en arrière**\n",
    "\n",
    "Fondamentalement, `backward` calcule l'erreur à partir de la sortie de `forward` et de la valeur réelle. Cette erreur est rétro-propagée à toutes les matrices de pondération en calculant les gradients de chaque couche et ces pondérations sont mises à jour. Regardons donc les méthodes de `backward` et `train`:\n",
    "\n",
    "Pour chaque itérations, nous appliquons l'algorithme back-prop, évaluons l'erreur et le gradient par rapport aux poids. Nous utilisons ensuite le taux d’apprentissage et les gradients pour mettre à jour les poids.\n",
    "\n",
    "Veuillez vous référer aux notes de cours pour l'explication complète des dérivés utilisés dans l'algorithme de back-prop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, nb_input_nodes, nb_hidden_nodes, nb_output_nodes, learning_rate):\n",
    "        #  >>> HYPERPARAMETERS <<<\n",
    "        self.nb_input_nodes = nb_input_nodes\n",
    "        self.nb_output_nodes = nb_output_nodes\n",
    "        self.nb_hidden_nodes = nb_hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = []\n",
    "\n",
    "        # >>> WEIGHTS + BIASES <<<\n",
    "        # Weight matrix from input to hidden layer\n",
    "        self.W1 = np.random.randn(self.nb_input_nodes, self.nb_hidden_nodes)\n",
    "        # Bias from input to hidden layer\n",
    "        self.b1 = np.ones((self.nb_hidden_nodes))\n",
    "\n",
    "        # Weight matrix from hidden to output layer\n",
    "        self.W2 = np.random.randn(self.nb_hidden_nodes, self.nb_output_nodes)\n",
    "        # Bias from hidden to output layer\n",
    "        self.b2 = np.ones((self.nb_output_nodes))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    def derivative_sigmoid(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def entropy_loss(self, y, y_pred):\n",
    "        # Prevent taking the log of 0\n",
    "        eps = np.finfo(float).eps\n",
    "        return -np.sum(y * np.log(y_pred + eps))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through our network\n",
    "        self.A1 = X\n",
    "        self.Z2 = np.dot(self.A1, self.W1) + self.b1\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        self.Z3 = np.dot(self.A2, self.W2) + self.b2\n",
    "        self.A3 = self.sigmoid(self.Z3)\n",
    "        return self.A3\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X_train.shape[0]  # number of examples\n",
    "\n",
    "        # Error in output\n",
    "        #d3\n",
    "        dZ3 = self.A3-y\n",
    "        # Delta for the weights w2\n",
    "        dW2 = (1./m) * np.dot(self.A2.T, dZ3)\n",
    "        # Delta for the bias b2\n",
    "        db2 = np.sum(dZ3, axis=0)  # sum across columns\n",
    "\n",
    "        # d2\n",
    "        dA2 = np.dot(dZ3, self.W2.T)\n",
    "        dZ2 = dA2 * self.derivative_sigmoid(self.Z2)\n",
    "        # Delta for the weights w1\n",
    "        dW1 = (1./m) * np.dot(X.T, dZ2)\n",
    "        # Delta for the bias b1\n",
    "        db1 = (1./m) * np.sum(dZ2, axis=0)  # sum across columns\n",
    "\n",
    "        # Wights and biases update\n",
    "        self.W2 = self.W2 - self.learning_rate * dW2\n",
    "        self.b2 = self.b2 - self.learning_rate * db2\n",
    "        self.W1 = self.W1 - self.learning_rate * dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * db1\n",
    "\n",
    "    def train (self, X, y, nb_iterations):\n",
    "        for i in range(nb_iterations):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.entropy_loss(y, y_pred)\n",
    "            self.loss.append(loss)\n",
    "            self.backward(X, y)\n",
    "\n",
    "            if i == 0 or i == nb_iterations-1:\n",
    "                print(f\"Iteration: {i+1}\")\n",
    "                print(tabulate(zip(X, y, [np.round(y_pred) for y_pred in self.A3] ), headers=[\"Input\", \"Actual\", \"Predicted\"]))\n",
    "                print(f\"Loss: {loss}\")                \n",
    "                print(\"\\n\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.round(self.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'ensemble d'entraînement on a 102 exemples et 8 primitives par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a aussi les étiquettes binaires pour les 102 exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de créer notre modèle et de le former, en utilisant 1000 itérations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(\n",
    "    nb_input_nodes = X_train.shape[1],  # 8 features\n",
    "    nb_hidden_nodes = 6,  # Number of neurons in the hidden layer\n",
    "    nb_output_nodes = len(classes),  # 5 classes we can use y_train.shape[1] too\n",
    "    learning_rate = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats de la première et de la dernière itération sont présentés comme une sortie de l'entraînement. Vous pouvez voir comment le classement s'améliore de l'itération 1, où pratiquement toutes les prédictions sont incorrectes, à 1000 où presque toutes sont correctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_iterations = 10000\n",
    "NN.train(X_train, y_train, nb_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons ci-dessous comment la fonction de perte se comporte à chaque itération :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NN.loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curve for training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous pouvons faire des prédictions avec le modèle entraîné en utilisant la fonction `predict` qui fait un passage en avant de l'ensemble des données de test en utilisant les poids et les biais déjà ajustés pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = NN.predict(X_test)\n",
    "print(tabulate(zip(X_test, labels_test, [classes.get(tuple(o), \"--\") for o in y_pred]), headers=[\"Input\", \"Actual\", \"Predicted\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
